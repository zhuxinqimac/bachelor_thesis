# Xinqi Zhu's Bachelor Thesis
Thesis title: Temporal Modeling in Action Recognition

# Abstract: 
Nowadays, action recognition in videos has become more and more popular in computer vision. In this thesis, we study the video based action recognition from the perspective of temporal modeling. Our motivation comes from some recent questions about temporal modeling, and our curiosity for better ways to exploit temporal information in videos. We first review the most promising deep learning based action recognition methods proposed within the last five years, and categorize them based on their main contributions for modeling temporal dynamics. Since we target at quantitatively describe a model’s capability of temporal modeling on a dataset, we propose our evaluation protocols to measure a model’s sensitivity about temporal coherency, and how temporal coherency influences a model’s training. The calculations of the proposed Sens-Index and Inf-Index do not require the revealing of labels. We then evaluate representative reviewed models using our protocols on UCF101 and Something-Something datasets, and provide our discussions and analyses. There are many meaningful and inspiring conclusions drawn from the experiments, and some are even counter-intuitive, e.g. LSTM models do not take much temporal information into account compared with other models; 3D Convolutions rely heavily on temporal coherency to do classification; we also find that a dataset’s requirement for temporal modeling can be roughly approximated using our Sens-Index. Besides investigating existing methods, we propose a novel model to exploit temporal information for action recognition, namely Approximated Bilinear Fusion. Our model is extended from bilinear pooling, adopting low-rank approximation technique to significantly reduce trainable parameters, and can model temporal dynamics by taking multiple frames as input. We show the derivation of the low-rank approximation, and how to effectively fuse across multiple frames. Finally, we present our exploration study on our proposed model, and validate our model’s performance and temporal modeling capacity on Something-Something and UCF101 datasets. The experimental results show that our model can outperform or compete with other methods, while keeping low complexity and fast convergence.

# Contact:
zhuxinqimac@outlook.com
